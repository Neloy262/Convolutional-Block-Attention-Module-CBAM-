{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CBAM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "-U6kcjwj3X5S"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelPool(nn.Module):\n",
        "\n",
        "  def forward(self,x):\n",
        "    return torch.cat([torch.unsqueeze(torch.max(x,1).values,1),torch.unsqueeze(torch.mean(x,1),1)],dim=1)"
      ],
      "metadata": {
        "id": "e3vPrRd-3nt6"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self,use_relu=False,kernel_size=7):\n",
        "        super(SpatialAttention,self).__init__()\n",
        "        \n",
        "        self.use_relu=use_relu\n",
        "        self.ch_pool = ChannelPool()\n",
        "        self.conv = nn.Conv2d(in_channels=2,out_channels=1,padding=\"same\",kernel_size=kernel_size)\n",
        "        self.batch_norm = nn.BatchNorm2d(1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        input_tensor = torch.clone(x)\n",
        "        x = self.ch_pool(x)\n",
        "        x = self.conv(x)\n",
        "\n",
        "        if self.use_relu:\n",
        "            x = self.relu(x)\n",
        "        \n",
        "        x = self.batch_norm(x)\n",
        "\n",
        "        attention_mask = self.sigmoid(x)\n",
        "        \n",
        "        return input_tensor*attention_mask"
      ],
      "metadata": {
        "id": "iafWgB_Z8Ipy"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "  \n",
        "      def __init__(self,c):\n",
        "          super(ChannelAttention, self).__init__()\n",
        "          self.Avg = nn.AdaptiveAvgPool2d((1,1))\n",
        "          self.Max = nn.AdaptiveMaxPool2d((1,1))\n",
        "\n",
        "          self.reduction_ratio = c//2 \n",
        "          self.fc1 = nn.Linear(c,self.reduction_ratio)\n",
        "          self.relu = nn.ReLU()\n",
        "          self.fc2 = nn.Linear(self.reduction_ratio,c)\n",
        "          self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "      def forward(self,x):\n",
        "          input_tensor = torch.clone(x)\n",
        "\n",
        "          x_avg = self.Avg(x)\n",
        "          x_avg = torch.flatten(x_avg,start_dim=1)\n",
        "          x_max = self.Max(x)\n",
        "          \n",
        "          \n",
        "          x_max = torch.flatten(x_max,start_dim=1)\n",
        "       \n",
        "          \n",
        "          \n",
        "          out1 = self.fc1(x_avg)\n",
        "         \n",
        "          out1 = self.relu(out1)\n",
        "          out1 = self.fc2(out1)\n",
        "\n",
        "\n",
        "          out2 = self.fc1(x_max)\n",
        "          out2 = self.relu(out2)\n",
        "          out2 = self.fc2(out2)\n",
        "\n",
        "          out = out1+out2\n",
        "\n",
        "          out = self.sigmoid(out)\n",
        "          out = torch.unsqueeze(torch.unsqueeze(out,-1),-1)\n",
        "\n",
        "          return input_tensor*out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6yHLyHTCuzER"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBAM(nn.Module):\n",
        "    def __init__(self,in_channels):\n",
        "     super(CBAM, self).__init__()\n",
        "     self.in_channels = in_channels\n",
        "     self.ch_attn = ChannelAttention(self.in_channels)\n",
        "     self.sp_attn = SpatialAttention()\n",
        "\n",
        "    def forward(self,x):\n",
        "       \n",
        "        out = self.ch_attn(x)\n",
        "        out = self.sp_attn(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZHU3asjZj2at"
      },
      "execution_count": 57,
      "outputs": []
    }
  ]
}